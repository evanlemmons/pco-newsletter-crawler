name: Newsletter Crawler

on:
  # Run daily at 6 AM UTC (adjust timezone as needed)
  schedule:
    - cron: '0 6 * * *'

  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      force_all:
        description: 'Crawl ALL sources regardless of frequency'
        required: false
        default: false
        type: boolean
      dry_run:
        description: 'Dry run (show what would be crawled)'
        required: false
        default: false
        type: boolean

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Safety timeout

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Run crawler
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GITHUB_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          # Build command with optional flags
          CMD="python newsletter_crawler.py"

          if [ "${{ github.event.inputs.force_all }}" == "true" ]; then
            CMD="$CMD --force-all"
          fi

          if [ "${{ github.event.inputs.dry_run }}" == "true" ]; then
            CMD="$CMD --dry-run"
          fi

          echo "Running: $CMD"
          $CMD
